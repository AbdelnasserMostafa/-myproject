{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_A-Z_Hands_On_Artificial_Neural_Network_course_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdelnasserMostafa/-myproject/blob/master/Deep_Learning_A_Z_Hands_On_Artificial_Neural_Network_course_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1R5OzyWDyMo"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#plt.grid(c='grey')\n",
        "plt.style.use('default')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtVLX0nwEjo7"
      },
      "source": [
        "# Welcome to Part 6 - AutoEncoders\n",
        "Section 24, Lecture 139\n",
        "Welcome to Part 6 - AutoEncoders\n",
        "\n",
        "\n",
        "\n",
        "In this part you will learn:\n",
        "\n",
        "1. the Intuition of AutoEncoders\n",
        "2. how to build an AutoEncoder from scratch with PyTorch\n",
        "3. how to manipulate classes and objects to improve and tune your AutoEncoder\n",
        "\n",
        "In the previous part we created a Recommender System that predicted binary ratings \"Like\" or \"Not Like\". In this part we will take it at the next level and create a Recommender System that predicts ratings from 1 to 5.\n",
        "\n",
        "We will implement a Stacked AutoEncoders model with PyTorch, a highly advanced Deep Learning platform more powerful than Keras. Every single line of code will be explained in details but I would recommend to have a first look at the PyTorch documentation to start getting familiar with PyTorch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOKbbkqbJAO6"
      },
      "source": [
        "# Training an Auto Encoder steps:\n",
        "\n",
        "STEP 1: We start with an array where the lines (the observations) correspond to the users and the columns (the features) correspond to the movies.  Each cell (u, i) contains the rating (from 1 to 5, 0 if no rating) of the movie i by the user u\n",
        "\n",
        "STEP 2: The first user goes into the network.  The input vector x = (r1, r2, ...., rm) contains all its ratings for all the movies.\n",
        "\n",
        "STEP 3: The input vector x is encoded into a vector z of lower dimensions by a mapping function f (e.g: sigmoid function):\n",
        "\n",
        "z = f(Wx + b) where W is the vector of input weights and b is the bias\n",
        "\n",
        "STEP 4: z is then decoded into the output vector y of same dimensions as x, aiming to replicate the input vector x.\n",
        "\n",
        "STEP 5: The reconstruction error d(x, y) = ||x-y|| is computed.  The goal is to minimize it.\n",
        "\n",
        "STEP 6: Back-Propagation: from right to left, the error is back-propagated.  The weights are updated according to how much they are responsible for the error.  The learning rate decides by how much we update the weights.\n",
        "\n",
        "STEP 7: Repeat Steps 1 to 6 and update the weights after each observation (This is Reinforcement Learning). Or: Repeat Steps 1 to 6 but update the weights only after a batch of observation (This is Batch Learning)\n",
        "\n",
        "STEP 8: When the whole training set passed through the ANN, that makes an epoch.  Redo more epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rdXaquQEGUn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f8a4cc4e-5ad9-4365-d4d9-9fee472809b3"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-30 06:28:43--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip.1’\n",
            "\n",
            "ml-100k.zip.1       100%[===================>]   4.70M  3.44MB/s    in 1.4s    \n",
            "\n",
            "2019-03-30 06:28:45 (3.44 MB/s) - ‘ml-100k.zip.1’ saved [4924029/4924029]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mwh2n1USTIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "02bf9506-6e82-4a84-81a3-b716fbae022b"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-30 06:29:07--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5917549 (5.6M) [application/zip]\n",
            "Saving to: ‘ml-1m.zip.1’\n",
            "\n",
            "ml-1m.zip.1         100%[===================>]   5.64M  4.12MB/s    in 1.4s    \n",
            "\n",
            "2019-03-30 06:29:08 (4.12 MB/s) - ‘ml-1m.zip.1’ saved [5917549/5917549]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPOy_18jSY0p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "37e67e70-7dd6-49fc-cae5-52fd696217f6"
      },
      "source": [
        "!unzip ml-100k.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ml-100k.zip\n",
            "replace ml-100k/allbut.pl? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTqOVWhRSdnA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a727bfc4-f3a4-432d-d823-fcd4b8dbc492"
      },
      "source": [
        "!unzip ml-1m.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ml-1m.zip\n",
            "replace ml-1m/movies.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ml-1m/movies.dat        \n",
            "  inflating: ml-1m/ratings.dat       \n",
            "  inflating: ml-1m/README            \n",
            "  inflating: ml-1m/users.dat         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZBRCbVLSliu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "bc821d4e-1e21-41f9-d30d-d9aee2f94be7"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Importing the dataset\n",
        "movies = pd.read_csv('/content/ml-1m/movies.dat', sep = '::', \n",
        "                     header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('/content/ml-1m/users.dat', sep = '::', \n",
        "                     header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('/content/ml-1m/ratings.dat', sep = '::', \n",
        "                     header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Preparing the training set and the test set\n",
        "training_set = pd.read_csv('/content/ml-100k/u1.base', delimiter = '\\t')\n",
        "test_set = pd.read_csv('/content/ml-100k/u1.test', delimiter = '\\t')\n",
        "\n",
        "# Convert the training_set and test_set to array\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')\n",
        "\n",
        "# Converting the data into an array with users in lines and movies in columns\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "# Converting the data into Torch tensors\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "\n",
        "# Creating the architecture of the Neural Network\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n",
        "\n",
        "# Training the SAE\n",
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
        "\n",
        "# Testing the SAE\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user])\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b6aa66ac577a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmean_corrector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_movies\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmean_corrector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6QhLNi0dQwX"
      },
      "source": [
        "# Data Preprocessing Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8lYMEUuS7pA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6a0cfd9b-9d41-4729-9486-434c8bd80a8f"
      },
      "source": [
        "# Downloading the Dataset\n",
        "\n",
        "!wget https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P16-Data-Preprocessing-Template.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-30 21:17:40--  https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P16-Data-Preprocessing-Template.zip\n",
            "Resolving sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)... 52.219.84.48\n",
            "Connecting to sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)|52.219.84.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3483 (3.4K) [application/zip]\n",
            "Saving to: ‘P16-Data-Preprocessing-Template.zip’\n",
            "\n",
            "\r          P16-Data-   0%[                    ]       0  --.-KB/s               \rP16-Data-Preprocess 100%[===================>]   3.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-30 21:17:41 (109 MB/s) - ‘P16-Data-Preprocessing-Template.zip’ saved [3483/3483]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1YBmalCdxEG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c8a2cf36-7a1b-485e-97e6-adf9175153a8"
      },
      "source": [
        "# Unzip the dataset\n",
        "\n",
        "!unzip P16-Data-Preprocessing-Template.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  P16-Data-Preprocessing-Template.zip\n",
            "   creating: Data Preprocessing Template/\n",
            "  inflating: Data Preprocessing Template/categorical_data.py  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/Data Preprocessing Template/\n",
            "  inflating: __MACOSX/Data Preprocessing Template/._categorical_data.py  \n",
            "  inflating: Data Preprocessing Template/Data.csv  \n",
            "  inflating: __MACOSX/Data Preprocessing Template/._Data.csv  \n",
            "  inflating: Data Preprocessing Template/data_preprocessing_template.py  \n",
            "  inflating: __MACOSX/Data Preprocessing Template/._data_preprocessing_template.py  \n",
            "  inflating: Data Preprocessing Template/missing_data.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iXtNaLWd638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2becb13c-2f0a-4495-bfe6-807bdd72b23f"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 32\n",
            "drwxr-xr-x 1 root root 4096 Mar 30 21:18  .\n",
            "drwxr-xr-x 1 root root 4096 Mar 30 21:14  ..\n",
            "drwxr-xr-x 1 root root 4096 Mar 27 20:25  .config\n",
            "drwxr-xr-x 2 root root 4096 Mar 31  2017 'Data Preprocessing Template'\n",
            "drwxrwxr-x 3 root root 4096 Mar 31  2017  __MACOSX\n",
            "-rw-r--r-- 1 root root 3483 Mar  6 17:48  P16-Data-Preprocessing-Template.zip\n",
            "drwxr-xr-x 1 root root 4096 Mar 27 20:26  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cHgmjhGd81E"
      },
      "source": [
        "!cd  'Data Preprocessing Template'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNB8v131eywt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "764deadf-40d1-401d-b12a-ba24e1f166bc"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZfOHyCZfscm"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/Data Preprocessing Template/Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, 3].values\n",
        "\n",
        "# Taking care of missing data\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
        "imputer = imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "\n",
        "# Encoding categorical data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
        "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "labelencoder_y = LabelEncoder()\n",
        "y = labelencoder_X.fit_transform(y)\n",
        "\n",
        "# Spliting the dataset into the Training set and Test set\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGydEgcdmZe1"
      },
      "source": [
        "# Classification Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko_Ex9gkgOWO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3X7hjXKjDUe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}